# DATA-STREAM-PLATFORM
Une plateforme Data Lakehouse temps rÃ©el conteneurisÃ©e, construite avec Kafka, Spark Streaming, Delta Lake (MinIO) et Streamlit


Real-Time Data Streaming & Lakehouse Platform
Ce dÃ©pÃ´t contient l'implÃ©mentation End-to-End et fonctionnelle d'une plateforme de donnÃ©es moderne, conÃ§ue pour ingÃ©rer, traiter et visualiser des flux de donnÃ©es en temps rÃ©el.

L'objectif de ce projet Ã©tait de construire une architecture scalable et rÃ©siliente capable de gÃ©rer une simulation de haute frÃ©quence (10k Ã©vÃ©nements/sec), en suivant les meilleures pratiques de l'industrie : le pattern Data Lakehouse.

ğŸ¯ Points ClÃ©s & FonctionnalitÃ©s
âš¡ Pipeline Ã‰vÃ©nementiel Complet : De la gÃ©nÃ©ration de donnÃ©es (Python Faker) Ã  l'ingestion via Apache Kafka.

ğŸ—ï¸ Architecture Lakehouse Robuste : Utilisation de Spark Structured Streaming pour Ã©crire des donnÃ©es en temps rÃ©el dans un format fiable et transactionnel (Delta Lake) sur du stockage objet S3 (MinIO). C'est la fondation de la "Bronze Layer".

âœ… Gouvernance & QualitÃ© de DonnÃ©e : ImplÃ©mentation d'un Stream Processor intermÃ©diaire pour valider les donnÃ©es Ã  la volÃ©e et router les erreurs vers une Dead Letter Queue (DLQ), garantissant que seules les donnÃ©es propres atteignent le Lakehouse.

ğŸ“Š Visualisation Temps RÃ©el : Un dashboard Streamlit connectÃ© directement au Lakehouse pour monitorer les KPIs d'ingestion.

ğŸ³ Infrastructure as Code (IaC) : La stack entiÃ¨re (6 services) est conteneurisÃ©e et orchestrÃ©e via un unique fichier Docker Compose, permettant un dÃ©ploiement local reproductible en une commande.
